import os
from pyspark.sql import SparkSession

# ğŸ”§ Caminho do arquivo Parquet
output_path = "/app/data/output.parquet"

# ğŸš€ Inicia Spark
spark = SparkSession.builder \
    .appName("SimpleSparkAppend") \
    .getOrCreate()

# ğŸ§¾ Novos dados a serem adicionados
new_data = [("Danillo", 4), ("Eveli", 5), ("Emili", 6)]
new_df = spark.createDataFrame(new_data, ["nome", "valor"])

if os.path.exists(output_path):
    print("âš ï¸ Arquivo jÃ¡ existe. Exibindo conteÃºdo atual:")
    existing_df = spark.read.parquet(output_path)
    existing_df.show()

    # ğŸ”„ Combina os dados antigos com os novos
    combined_df = existing_df.union(new_df)
else:
    print("ğŸ“ Arquivo nÃ£o existe. Criando novo com os dados.")
    combined_df = new_df

# ğŸ’¾ Salva os dados combinados sobrescrevendo o arquivo
combined_df.write.mode("overwrite").parquet(output_path)
print("âœ… Dados atualizados e salvos com sucesso.")

# ğŸ‘€ Exibe os dados finais
print("ğŸ“Š Dados finais no arquivo:")
final_df = spark.read.parquet(output_path)
final_df.show()

# ğŸ§¹ Encerra Spark
spark.stop()
