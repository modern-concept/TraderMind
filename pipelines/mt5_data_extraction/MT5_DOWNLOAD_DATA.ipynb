{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9551190c-3ad8-48dd-ae03-c6ed6c14f70c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize SparkSession\u001b[39;00m\n\u001b[0;32m     12\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMT5_Spread_Data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch_symbol_data_with_spread\u001b[39m(start_date, end_date, pairs, interval_days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet_output\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Initialize MetaTrader 5 platform\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mt5\u001b[38;5;241m.\u001b[39minitialize():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\core\\context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\core\\context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m     )\n\u001b[1;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    208\u001b[0m         master,\n\u001b[0;32m    209\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    220\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\core\\context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\java_gateway.py:108\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 108\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    112\u001b[0m         errorClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    113\u001b[0m         messageParameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    114\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import MetaTrader5 as mt5\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import duckdb\n",
    "import os\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MT5_Spread_Data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def fetch_symbol_data_with_spread(start_date, end_date, pairs, interval_days=30, output_dir=\"parquet_output\"):\n",
    "    # Initialize MetaTrader 5 platform\n",
    "    if not mt5.initialize():\n",
    "        print(\"Failed to initialize MetaTrader 5\")\n",
    "        return\n",
    "\n",
    "    interval = timedelta(days=interval_days)\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for symbol, contract in pairs:\n",
    "        all_data = []\n",
    "        current_date = start_date\n",
    "\n",
    "        while current_date < end_date:\n",
    "            interval_end = min(current_date + interval, end_date)\n",
    "            print(f\"Fetching data for {symbol} from {current_date} to {interval_end}\")\n",
    "\n",
    "            # Get 1-minute candles\n",
    "            rates = mt5.copy_rates_range(symbol, mt5.TIMEFRAME_M1, current_date, interval_end)\n",
    "\n",
    "            if rates is None or len(rates) == 0:\n",
    "                print(f\"No candle data for {symbol} between {current_date} and {interval_end}\")\n",
    "            else:\n",
    "                # Get ticks to calculate spreads\n",
    "                ticks = mt5.copy_ticks_range(symbol, current_date, interval_end, mt5.COPY_TICKS_ALL)\n",
    "                tick_spreads = {}\n",
    "\n",
    "                if ticks is not None and len(ticks) > 0:\n",
    "                    for tick in ticks:\n",
    "                        tick_time = datetime.fromtimestamp(tick['time']).strftime('%Y.%m.%d %H:%M')\n",
    "                        tick_spreads.setdefault(tick_time, {'ask': [], 'bid': []})\n",
    "                        tick_spreads[tick_time]['ask'].append(tick['ask'])\n",
    "                        tick_spreads[tick_time]['bid'].append(tick['bid'])\n",
    "\n",
    "                    # Average spread per minute\n",
    "                    for tick_time, prices in tick_spreads.items():\n",
    "                        avg_ask = sum(prices['ask']) / len(prices['ask'])\n",
    "                        avg_bid = sum(prices['bid']) / len(prices['bid'])\n",
    "                        tick_spreads[tick_time] = avg_ask - avg_bid\n",
    "\n",
    "                # Combine candle and spread data\n",
    "                for rate in rates:\n",
    "                    time_str = datetime.fromtimestamp(rate['time']).strftime('%Y.%m.%d %H:%M:%S')\n",
    "                    minute_key = datetime.fromtimestamp(rate['time']).strftime('%Y.%m.%d %H:%M')\n",
    "                    spread = contract * tick_spreads.get(minute_key, rate['high'] - rate['low'])\n",
    "\n",
    "                    all_data.append({\n",
    "                        \"SYMBOL\": symbol,\n",
    "                        \"TIME\": time_str,\n",
    "                        \"OPEN\": rate['open'],\n",
    "                        \"HIGH\": rate['high'],\n",
    "                        \"LOW\": rate['low'],\n",
    "                        \"CLOSE\": rate['close'],\n",
    "                        \"TICK_VOLUME\": rate['tick_volume'],\n",
    "                        \"SPREAD\": spread\n",
    "                    })\n",
    "\n",
    "            current_date = interval_end\n",
    "\n",
    "        # Convert to Spark DataFrame\n",
    "        df = pd.DataFrame(all_data)\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "\n",
    "        # Save as Parquet file\n",
    "        parquet_path = f\"{output_dir}/{symbol.lower()}_spread_data.parquet\"\n",
    "        spark_df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "        print(f\"✔ Spark wrote data for {symbol} to {parquet_path}\")\n",
    "\n",
    "        # Load into DuckDB as a table\n",
    "        with duckdb.connect(\"mt5_data.duckdb\") as con:\n",
    "            con.execute(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS spread_data_{symbol.lower()} AS\n",
    "                SELECT * FROM read_parquet('{parquet_path}')\n",
    "            \"\"\")\n",
    "            print(f\"✔ DuckDB table 'spread_data_{symbol.lower()}' created from Parquet\")\n",
    "\n",
    "    # Shutdown MetaTrader\n",
    "    mt5.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e6563-3871-4d21-bfc5-7732bb4be891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the start and end dates for historical data collection\n",
    "start = datetime(2024, 12, 30, 0, 0)\n",
    "end = datetime(2024, 12, 31, 0, 0)\n",
    "\n",
    "# List of trading pairs with their respective contract size\n",
    "assets = [\n",
    "    (\"GOLD\", 100),       # Gold contract: 100 ounces\n",
    "    (\"EURUSD\", 100000),  # EUR/USD contract: 100,000 units\n",
    "    (\"BTCUSD\", 1)        # Bitcoin contract: 1 BTC (example)\n",
    "]\n",
    "\n",
    "# Number of days for each interval query\n",
    "interval_days = 30\n",
    "\n",
    "# Output folder where Spark will write Parquet files\n",
    "output_dir = \"parquet_output\"\n",
    "\n",
    "# Function call with parameters\n",
    "fetch_symbol_data_with_spread(\n",
    "    start_date=start,\n",
    "    end_date=end,\n",
    "    pairs=assets,\n",
    "    interval_days=interval_days,\n",
    "    output_dir=output_dir\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
